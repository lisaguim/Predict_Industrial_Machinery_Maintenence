<h2 align="center"> Predict Industrial Machinery Maintenence </h1>

### üìå Project description and methodology

The goal of this project is to develop a supervised learning binary classification algorithm to predict when maintenance is required or not on an industrial machine.

The methodology of this project consists of the steps of data exploration and cleaning, data preprocessing, modeling, model evaluation and optimization.

### üìú Dataset

The dataset used contains fictitious data from the recording of 178 IoT (Internet of Things) sensor data readings.

Each row of the dataset contains 178 readings, which are the columns of different sensors. In other words, there are 178 columns that make up the IoT sensor readings. In total, there are 11,500 rows and 179 columns, with the last column (179th) containing the status of the industrial machine, whether or not it required maintenance, i.e., it is the target variable.

### üìö Libraries

Python libraries used for this project are:

- pickle
- sklearn
- pandas
- numpy
- seaborn
- matplotlib
- xgboost
- spicy  

### Explory Analysis and Data Cleanning

In this step, we checked null & duplicate values in dataset and  positive class prevalence.

### ‚öôÔ∏è Pre-Processing

  - Data Division at trainning, validation and test: 

  The dataset was divided into training, validation and test subsets (70/15/15). These subsets were generated by random sampling (since this is not a time series). A random sample of 30% of the original data was extracted, which was later 
  separated into df_test and df_valid. And the remaining 70% corresponds to df_train. In this way, it is possible to maintain a positive prevalence of ~20% in each subset.

  Subset:
  
     df_train = train machine learning algorithm and represents the majority of dataset volume
     df_test = will be used to adjust hyperparameters and select best perform
     df_valid = will be used to test the accuracy of ML algorithm

  - Class balancing (trainning subset): 

  It was checked that dataset had a balacing class of 80% negative values versus 20% positive values. Therefore, class balancing was applied for trainning dataset through undersampling technique.

  After that, class balancing is 50% and train dataset reduced from 8050 to 3248 rows (samples).

  - Standardization:

In this step, the data from the columns (features) of the training and validation sets are standardized (or normalized) using the StandardScaler class from the sklearn library.

###  üóëÔ∏è Data Modeling Predictive

  - Metrics:

Metric functions were defined to evaluate the three predictive model options.
Since this is a binary classification model, the metrics used were AUC (Area Under the Curve), Accuracy, Recall, Precision and Specificity.
  
  - Creating mmodels:

When determining the machine learning classification algorithm, three prediction algorithm options will be created: Logistic Regression, Gaussian Naive Bayes, and Decision Tree with Boosting.
The objective of this step is to evaluate which model is best (based on the metrics) for this project.

### üìó Cross Validation

The best model so far is the third option (Gaussian Naive Bayes) based on the metrics evaluations, but it is still necessary to verify its generalization capacity, that is, if the model understood the mathematical relationship between the data and not only the details of the training data. Therefore, in this step, we be used the cross validation to check if model is overfitting or not.

### ‚öñÔ∏è Hypermarameter Optimization

As everything can be improved, this step aims to optimize the xgboosting hyperparameter for model 3, in order to achieve the best model performance through GridSearchCV from sklearn.

### üìä Results

Based on the analysis of the data, the optimized XGBoost prediction model is the best model as it presented the best performance and evaluation in the AUC metric.

